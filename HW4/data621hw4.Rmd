---
title: "data621hw4temp"
author: "jim lung"
date: "April 17, 2018"
output:
  word_document: default
  pdf_document: default
  tidy: yes
  html_document:
    highlight: pygments
    theme: cerulean
code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


* * * 


## 1. DATA EXPLORATION (25 Points)
Describe the size and the variables in the insurance training data set. Consider that too much detail will cause a
manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment.
You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median

b. Bar Chart or Box Plot of the data

c. Is the data correlated to the target variable (or to other variables?)

d. Are any of the variables missing and need to be imputed “fixed”?

***

a. Mean / Standard Deviation / Median

```{r, message=FALSE, warning=FALSE}
require("plyr")
require("knitr")
require("psych")
# Let's load the data

training <- read.csv(url('https://raw.githubusercontent.com/fung1091/data621/master/HW4/insurance_training_data.csv'),stringsAsFactors = FALSE)

evaluation <- read.csv(url('https://raw.githubusercontent.com/fung1091/data621/master/HW4/insurance-evaluation-data.csv'),stringsAsFactors = FALSE)


columns <- colnames(training)
target <- "TARGET_FLAG"
inputs <- columns[!columns %in% c(target,"INDEX")]


summary <- describe(training[,c(target,inputs)],na.rm = TRUE)[,c("n","mean","sd","median","min","max")]
summary$completeness <- summary$n/nrow(training)
summary$cv <- 100*summary$sd/summary$mean

kable(summary)

head(training)
summary(training)
```

## 2. DATA PREPARATION (25 Points)

Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)

b. Create flags to suggest if a variable was missing

c. Transform data by putting it into buckets

d. Mathematical transforms such as log or square root (or use Box-Cox)

e. Combine variables (such as ratios or adding or multiplying) to create new variables

### Data Transformations

Based on the dataset description we need to:

 * Convert INCOME to numeric, replace 0 for NA
 * Convert PARENT1 to flag (1/0)
 * Convert HOME_VAL to NON_HOMEOWNER flag
 * Convert MSTATUS to Flag IS_SINGLE (1/0)
 * Convert SEX to Flag (IS_MALE)
 * Breakout EDUCATION into ED_HS, ED_BACHELORS,ED_MASTERS, ED_PHD
 * Breakout JOB into JOB_BLUE_COLLAR, JOB_CLERICAL, JOB_PROFESSIONAL, JOB_MANAGERIAL, JOB_LAWYER, JOB_STUDENT,JOB_DOCTOR, JOB_HOME_MAKER
 * Convert CAR_USE to flat(1/0 IS_COMMERCIAL)
 * Convert BLUEBOOK to numeric
 * Breakout CAR_TYPE into: CAR_PANEL_TRUCK,CAR_PICKUP,CAR_SPORTS_CAR,CAR_VAN,CAR_SUV
 * Convert RED_CAR to flag (1/0)
 * Convert OLDCLAIM to numeric   
 * Convert REVOKED to flag (1/0)
 * Convert URBANICITY to flag (1/0 IS_URBAN)
 
 As a convention, all binary variables will be prefixed with "_BIN"

 
```{r}

parseStringValue <- function(v, zeroToNa){
  tmpVal <- as.numeric(gsub("[\\$,]","", v))
  if (!is.na(tmpVal) && tmpVal == 0 && zeroToNa) { NA } else {tmpVal}
}

transform <- function(d){
  outputCols<- c("TARGET_FLAG","TARGET_AMT","AGE", "YOJ", "CAR_AGE","KIDSDRIV","HOMEKIDS","TRAVTIME","TIF","CLM_FREQ","MVR_PTS")
  

  #* Convert INCOME to numeric, replace 0 for NA
  d['INCOME'] <- parseStringValue(d['INCOME'],TRUE)
  outputCols <- c(outputCols,'INCOME')
 
  #* Convert PARENT1 to flag (1/0)
  d['PARENT1_BIN'] <- if (d['PARENT1']=="Yes") {1} else {0}
  outputCols <- c(outputCols,'PARENT1_BIN')

  #* Convert HOME_VAL to NON_HOMEOWNER flag
  d['NON_HOMEOWNER_BIN'] <- if (is.na(parseStringValue(d['HOME_VAL'],TRUE))) {1} else {0}
  outputCols <- c(outputCols,'NON_HOMEOWNER_BIN')
  
  #* Convert MSTATUS to Flag  IS_SINGLE (1/0
  #levels(training$MSTATUS)
  d['IS_SINGLE_BIN'] <- if (d['MSTATUS']=="z_No") {1} else {0}
  outputCols <- c(outputCols,'IS_SINGLE_BIN')

  #* Convert SEX to Flag (IS_MALE)
  d['IS_MALE_BIN'] <- if (d['SEX']=="M") {1} else {0}
  outputCols <- c(outputCols,'IS_MALE_BIN')
  
  #* Breakout EDUCATION into ED_HS, ED_BACHELORS,ED_MASTERS, ED_PHD
  d['ED_HS_BIN'] <- if (d['EDUCATION']=="z_High School") {1} else {0}
  d['ED_BACHELORS_BIN'] <- if (d['EDUCATION']=="Bachelors") {1} else {0}
  d['ED_MASTERS_BIN'] <- if (d['EDUCATION']=="Masters") {1} else {0}
  d['ED_PHD_BIN'] <- if (d['EDUCATION']=="PhD") {1} else {0}
  outputCols <- c(outputCols,'ED_HS_BIN','ED_BACHELORS_BIN','ED_MASTERS_BIN','ED_PHD_BIN')

  #* Breakout JOB into JOB_BLUE_COLLAR, JOB_CLERICAL, JOB_PROFESSIONAL, JOB_MANAGERIAL, JOB_LAWYER, JOB_STUDENT, JOB_DOCTOR, JOB_HOME_MAKER
  d['JOB_BLUE_COLLAR_BIN'] <- if (d['JOB']=="z_Blue Collar") {1} else {0}
  d['JOB_CLERICAL_BIN'] <- if (d['JOB']=="Clerical") {1} else {0}
  d['JOB_PROFESSIONAL_BIN'] <- if (d['JOB']=="Professional") {1} else {0}
  d['JOB_MANAGERIAL_BIN'] <- if (d['JOB']=="Manager") {1} else {0}
  d['JOB_LAWYER_BIN'] <- if (d['JOB']=="Lawyer") {1} else {0}
  d['JOB_STUDENT_BIN'] <- if (d['JOB']=="Student") {1} else {0}
  d['JOB_DOCTOR_BIN'] <- if (d['JOB']=="Doctor") {1} else {0}
  d['JOB_HOME_MAKER_BIN'] <- if (d['JOB']=="Home Maker") {1} else {0}
  outputCols <- c(outputCols,'JOB_BLUE_COLLAR_BIN', 'JOB_CLERICAL_BIN', 'JOB_PROFESSIONAL_BIN', 'JOB_MANAGERIAL_BIN', 'JOB_LAWYER_BIN', 'JOB_STUDENT_BIN', 'JOB_DOCTOR_BIN', 'JOB_HOME_MAKER_BIN')

  #* Convert CAR_USE to flat(1/0 IS_COMMERCIAL)
  #levels(training$CAR_USE)
  d['IS_COMMERCIAL_BIN'] <- if (d['CAR_USE']=="Commercial") {1} else {0}
  outputCols <- c(outputCols,'IS_COMMERCIAL_BIN')
  
  
  #* Convert BLUEBOOK to numeric
  d['BLUEBOOK'] <- parseStringValue(d['BLUEBOOK'],FALSE)
  outputCols <- c(outputCols,'BLUEBOOK')

  #* Breakout CAR_TYPE into: CAR_PANEL_TRUCK,CAR_PICKUP,CAR_SPORTS_CAR,CAR_VAN,CAR_SUV
  #levels(training$CAR_TYPE)
  d['CAR_PANEL_TRUCK_BIN'] <- if (d['CAR_TYPE']=="Panel Truck") {1} else {0}
  d['CAR_PICKUP_BIN'] <- if (d['CAR_TYPE']=="Pickup") {1} else {0}
  d['CAR_SPORTS_CAR_BIN'] <- if (d['CAR_TYPE']=="Sports Car") {1} else {0}
  d['CAR_VAN_BIN'] <- if (d['CAR_TYPE']=="Van") {1} else {0}
  d['CAR_SUV_BIN'] <- if (d['CAR_TYPE']=="z_SUV") {1} else {0}
  outputCols <- c(outputCols,'CAR_PANEL_TRUCK_BIN','CAR_PICKUP_BIN','CAR_SPORTS_CAR_BIN','CAR_VAN_BIN','CAR_SUV_BIN')

  #* Convert RED_CAR to flag (1/0)
  #levels(training$RED_CAR)
  d['RED_CAR_BIN'] <- if (d['RED_CAR']=="yes") {1} else {0}
  outputCols <- c(outputCols,'RED_CAR_BIN')
  
  #* Convert OLDCLAIM to numeric
  #levels(training$OLDCLAIM)
  d['OLDCLAIM'] <- parseStringValue(d['OLDCLAIM'],TRUE)
  outputCols <- c(outputCols,'OLDCLAIM')
  
  #* Convert REVOKED to flag (1/0)
  #levels(training$REVOKED)
  d['REVOKED_BIN'] <- if (d['REVOKED']=="Yes") {1} else {0}
  outputCols <- c(outputCols,'REVOKED_BIN')
  
  #* Convert URBANICITY to flag (1/0 IS_URBAN)
  #levels(training$URBANICITY)
  d['IS_URBAN_BIN'] <- if (d['URBANICITY']=="Highly Urban/ Urban") {1} else {0}
  outputCols <- c(outputCols,'IS_URBAN_BIN')
  
   
  r <- as.numeric(d[outputCols])
  names(r) <- outputCols
  r
} 

# form dataframe by function
training_trans<-data.frame(t(rbind(apply(training,1,transform))))
evaluation_trans<-data.frame(t(rbind(apply(evaluation,1,transform))))

columns <- colnames(training_trans)
target_bin <- c("TARGET_FLAG")
target_lm <- c("TARGET_AMT")
target <- c(target_bin,target_lm)
inputs_bin <- columns[grep("_BIN",columns)]
inputs_num <- columns[!columns %in% c(target,"INDEX",inputs_bin)]
inputs<- c(inputs_bin,inputs_num)


```

### Data Imputations

#### Imputations

 * Fill missing nummerical values with mean for: AGE, YOJ, CAR_AGE, INCOME
 * Impute missing OLDCLAIM with zeros

```{r}
# impute
impute <- function (d) {
  d[is.na(d$AGE),]$AGE <- mean(d$AGE,na.rm = TRUE)
  d[is.na(d$YOJ),]$YOJ <- mean(d$YOJ,na.rm = TRUE)
  d[is.na(d$CAR_AGE),]$CAR_AGE <- mean(d$CAR_AGE,na.rm = TRUE)
  d[is.na(d$INCOME),]$INCOME <- mean(d$INCOME,na.rm = TRUE)
  d[is.na(d$OLDCLAIM),]$OLDCLAIM <- 0
  d
}
training_trans<-impute(training_trans)
evaluation_trans<-impute(evaluation_trans)

```


#### Transformation Analysis 


##### **TARGET_NUM**

```{r}
hist(training_trans[training_trans$TARGET_FLAG==1,target_lm])
```

The distribution of values of the response target_lm suggest that we may benefit from a log tranformation on the response.


For better linear pattern, we should get a better linear fit. A log transformation of the target seems adequate, aside from some negative values that need to be zeroed out, it is not evident that any outliers of the predictors may skew the linear fit. With that, no further transformations seem required. 

##### **Transformations Implementation**

Numerical Transformations:

* Cap AGE at 70, negative values not permitted
* Cap YOJ at 20, negative values not permitted
* Cap CAR_AGE at 20, negative values not permitted
* Cap KIDSDRIV at 3, negative values not permitted
* Cap HOMEKIDS at 4, negative values not permitted
* Cap TRAVTIME at 75, negative values not permitted
* Cap TIF at 17, negative values not permitted
* Cap CLM_FREQ at 4, negative values not permitted
* Cap MVR_PTS at 10, negative values not permitted
* Cap INCOME at 175000, negative values not permitted
* Cap BLUEBOOK at 40000, negative values not permitted
* Cap OLDCLAIM at 40000, negative values not permitted


```{r}

# Cap values

d<- training_trans 
capColumns <- function(d){
  outputCols<- colnames(d)
  

  #* Cap AGE at 70, negative values not permitted
  d[d$AGE <0, 'AGE'] <- 0
  d[d$AGE >=70, 'AGE'] <- 70
  
  #* Cap YOJ at 20, negative values not permitted
  d[d$YOJ <0, 'YOJ'] <- 0
  d[d$YOJ >=20, 'YOJ'] <- 20
  
  #* Cap CAR_AGE at 20, negative values not permitted
  d[d$CAR_AGE <0, 'CAR_AGE'] <- 0
  d[d$CAR_AGE >=20, 'CAR_AGE'] <- 20
  
  #* Cap KIDSDRIV at 3, negative values not permitted
  d[d$KIDSDRIV <0, 'KIDSDRIV'] <- 0
  d[d$KIDSDRIV >=3, 'KIDSDRIV'] <- 3
  
  #* Cap HOMEKIDS at 4, negative values not permitted
  d[d$HOMEKIDS <0, 'HOMEKIDS'] <- 0
  d[d$HOMEKIDS >=4, 'HOMEKIDS'] <- 4

  #* Cap TRAVTIME at 75, negative values not permitted
  d[d$TRAVTIME <0, 'TRAVTIME'] <- 0
  d[d$TRAVTIME >=75, 'TRAVTIME'] <- 75

  #* Cap TIF at 17, negative values not permitted
  d[d$TIF <0, 'TIF'] <- 0
  d[d$TIF >=17, 'TIF'] <- 17

  #* Cap CLM_FREQ at 4, negative values not permitted
  d[d$CLM_FREQ <0, 'CLM_FREQ'] <- 0
  d[d$CLM_FREQ >=4, 'CLM_FREQ'] <- 4

  #* Cap MVR_PTS at 10, negative values not permitted
  d[d$MVR_PTS <0, 'MVR_PTS'] <- 0
  d[d$MVR_PTS >=10, 'MVR_PTS'] <- 10

  #* Cap INCOME at 175000, negative values not permitted
  d[d$INCOME <0, 'INCOME'] <- 0
  d[d$INCOME >=175000, 'INCOME'] <- 175000

  #* Cap BLUEBOOK at 40000, negative values not permitted
  d[d$BLUEBOOK <0, 'BLUEBOOK'] <- 0
  d[d$BLUEBOOK >=40000, 'BLUEBOOK'] <- 40000
  
  #* Cap OLDCLAIM at 40000, negative values not permitted
  d[d$OLDCLAIM <0, 'OLDCLAIM'] <- 0
  d[d$OLDCLAIM >=40000, 'OLDCLAIM'] <- 40000
  
  d

}


training_trans <- capColumns(training_trans)
evaluation_trans <- capColumns(evaluation_trans)




```

#### Final summary

```{r}
summary <- describe(training_trans[,c(target,inputs)])[,c("n","mean","sd","median","min","max")]
summary$completeness <- summary$n/nrow(training_trans)
summary$cv <- 100*summary$sd/summary$mean

kable(summary)

#head(training_trans)
#summary(training_trans)


```

### distribution of the values for each of the variables

Here's the distribution of the values for each of the variables

we get a view of the normalized values:

# Binary target variable

```{r}
head(data.frame(scale(training_trans[,inputs_num])))
```

## Boxplot of Target Flag vs Numerical Predictors and Target Flag vs Binary Predictors

```{r,warning=FALSE,message=FALSE}
require("reshape2")
require("ggplot2")
detach(package:plyr)
require("dplyr")

# Let's melt the DF so that we can plot it more easily
training_normalized <- cbind(data.frame(scale(training_trans[,inputs_num])),training_trans[,c(inputs_bin,target)])
training_normalized$TARGET_FLAG <- training_normalized$TARGET_FLAG==1

ggplot(melt(training_normalized, measure.vars = inputs_num),
       aes(x=variable,y=value)
       )+
    geom_boxplot(aes(fill = factor(TARGET_FLAG)))+
  guides(fill=guide_legend(title="Was Car in a crash")) +
   theme(legend.position="bottom")+
    coord_flip()+
  labs(title="Boxplot of Target Flag ~ Numerical Predictors", y="Normalized Values", x="Predictor")

bin_summary <- melt(training_normalized[,c(inputs_bin,target_bin)], measure.vars = inputs_bin) %>%
  group_by(TARGET_FLAG,variable) %>%
  summarise(pct = sum(value)/length(value))

ggplot(bin_summary, aes(variable, pct)) +   
  geom_bar(aes(fill = TARGET_FLAG), position = "dodge", stat="identity")+
  guides(fill=guide_legend(title="Was Car in a crash")) +
   theme(legend.position="bottom")+
    coord_flip()+
  labs(title="Boxplot of Target Flag ~ Binary Predictors", y="Percent", x="Predictor")

```

## Correlations

```{r warning=FALSE, message=FALSE}

summary_positive <- describe(training_normalized[training_normalized$TARGET_FLAG==1,c(target_bin,inputs)])[,c("mean","n")]
summary_negative <- describe(training_normalized[training_normalized$TARGET_FLAG==0,c(target_bin,inputs)])[,c("mean","n")]
summary_by_target <- merge(summary_positive,summary_negative,by="row.names")
colnames(summary_by_target) <- c("Variable","In car crash - Avg","In car crash - n","NOT In car crash - Avg", "NOT In car crash - n")
summary_by_target$delta <- abs(summary_by_target[,"In car crash - Avg"]-summary_by_target[,"NOT In car crash - Avg"])

kable(summary_by_target[order(-summary_by_target$delta),])

```



# Numerical target variable - Cost of Car Crash

For our descriptive stats & intuitive understanding, let's discretize the car crash into Above / Below median cost

```{r}


# Let's melt the DF so that we can plot it more easily
training_normalized<-training_normalized[training_normalized$TARGET_FLAG,]

training_normalized$TARGET_FLAG <- training_normalized$TARGET_AMT > median(training_normalized$TARGET_AMT)

ggplot(melt(training_normalized, measure.vars = inputs_num),
       aes(x=variable,y=value)
       )+
    geom_boxplot(aes(fill = factor(TARGET_FLAG)))+
  guides(fill=guide_legend(title="Cost of Car Crash Above Median")) +
   theme(legend.position="bottom")+
    coord_flip()+
  labs(title="Boxplot of Cost of Car Crash Above Median ~ Numerical Predictors", y="Normalized Values", x="Predictor")

bin_summary <- melt(training_normalized[,c(inputs_bin,target_bin)], measure.vars = inputs_bin) %>%
  group_by(TARGET_FLAG,variable) %>%
  summarise(pct = sum(value)/length(value))

ggplot(bin_summary, aes(variable, pct)) +   
  geom_bar(aes(fill = TARGET_FLAG), position = "dodge", stat="identity")+
  guides(fill=guide_legend(title="Cost of Car Crash Above Median")) +
   theme(legend.position="bottom")+
    coord_flip()+
  labs(title="Boxplot of Cost of Car Crash Above Median ~ Binary Predictors", y="Percent", x="Predictor")


```

## correlations

```{r}


summary_positive <- describe(training_normalized[training_normalized$TARGET_FLAG==1,c(target_bin,inputs)])[,c("mean","n")]
summary_negative <- describe(training_normalized[training_normalized$TARGET_FLAG==0,c(target_bin,inputs)])[,c("mean","n")]
summary_by_target <- merge(summary_positive,summary_negative,by="row.names")
colnames(summary_by_target) <- c("Variable","Above Median Cost of Crash - Avg","Above Median Cost of Crash - n","Below Median Cost of Crash - Avg", "Below Median Cost of Crash - n")
summary_by_target$delta <- abs(summary_by_target[,"Above Median Cost of Crash - Avg"]-summary_by_target[,"Below Median Cost of Crash - Avg"])

kable(summary_by_target[order(-summary_by_target$delta),])

```


## TRAINIG DATASETS

#NEED TO:

* split datasets 
* run models


```{r}

library(caTools)

train_rows <- sample.split(training_trans$TARGET_FLAG, SplitRatio=0.7)
training_trans_model_bin <- training_trans[train_rows,]
training_trans_eval_bin <- training_trans[-train_rows,]

```

## 3. BUILD MODELS (25 Points)

Using the training data set, build at least two different multiple linear regression models and three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.

Be sure to explain how you can make inferences from the model, as well as discuss other relevant model output. Discuss the coefficients in the models, do they make sense? Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.

#### MODEL 1. 

MLR Full model, all variables, flag + amt

The flag one looks okay here, the amt one doesn't seem to work so well.

```{r}
training_target_amt <- training_trans[training_trans$TARGET_FLAG==1,]
target_amt_model_all <- glm(TARGET_AMT~.,data=training_target_amt[,c(inputs,target_lm)])
predict1 <- round(predict(target_amt_model_all, training_trans_eval_bin, type = 'response'), 4)
summary(target_amt_model_all)
model1_amt <- target_amt_model_all
```


#### MODEL 2.

MLR Full model with log transformation on amt, all variables, amt only

```{r}
training_target_amt$TARGET_AMT <- log(training_target_amt$TARGET_AMT)
target_amt_model_all <- glm(TARGET_AMT~.,data=training_target_amt[,c(inputs,target_lm)])
predict2 <- round(predict(target_amt_model_all, training_trans_eval_bin, type = 'response'), 4)
summary(target_amt_model_all)
model2_amt <- target_amt_model_all
```


#### Model 3.

Manually remove variables from model 1 that weren't significant for flag. And try a version for amt that only has a few variables.

```{r}
inputs_manual_amt <- inputs[c(4,24,28,36)]
training_target_amt <- training_trans[training_trans$TARGET_FLAG==1,]
target_amt_model_all <- glm(TARGET_AMT~.,data=training_target_amt[,c(inputs_manual_amt,target_lm)])
predict3 <- round(predict(target_amt_model_all, training_trans_eval_bin, type = 'response'), 4)
summary(target_amt_model_all)
model3_amt = target_amt_model_all
```


#### Model 4.

Binary Logistic Regression Baseline with all variables.

```{r}
training_target_flag <- training_trans_model_bin
target_flag_model_all <- glm(TARGET_FLAG~.,data=training_target_flag[,c(inputs,target_bin)],family = binomial(link = "logit"))
predict4 <- round(predict(target_flag_model_all, training_trans_eval_bin, type = 'response'), 4)
summary(target_flag_model_all)
model4_flag = target_flag_model_all
```

#### Model 5.

```{r}
inputs_manual_flag <- inputs[-c(4,5,8,9,11,13,14,15,23,26,28,30)]
target_flag_model_all <- glm(TARGET_FLAG~.,data=training_target_flag[,c(inputs_manual_flag,target_bin)],family = binomial(link = "logit"))
predict5 <- round(predict(target_flag_model_all, training_trans_eval_bin, type = 'response'), 4)
summary(target_flag_model_all)
model5_flag = target_flag_model_all
```

#### Model 6.

```{r, cache=TRUE}
stepwise_flag_model <- glm(TARGET_FLAG~.,data=training_target_flag[,c(inputs,target_bin)], family = binomial(link = "probit"))

backward <- step(stepwise_flag_model, trace = 0)
predict6 <- round(predict(backward,training_trans_eval_bin , type = 'response'), 4)
summary(backward)
model6_flag <- backward
```

#### Model 7.

```{r cache = TRUE}
stepwise_flag_model2 <- glm(TARGET_FLAG~1,data=training_target_flag[,c(inputs,target_bin)], family = binomial(link = "probit"))

forward <- step(stepwise_flag_model2, scope = list(lower=formula(stepwise_flag_model2), upper=formula(stepwise_flag_model)), direction = "forward", trace = 0)
predict7 <- round(predict(forward, training_trans_eval_bin ,type = 'response'), 4)
summary(forward)
model7_flag <- forward
```



## 4. SELECT MODELS (25 Points)

Decide on the criteria for selecting the best multiple linear regression model and the best binary logistic regression model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models.

For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.

```{r}
par(mfrow=c(2,2))
plot(model1_amt)
plot(model2_amt)
plot(model3_amt)
plot(model4_flag)
plot(model5_flag)
plot(model6_flag)
plot(model7_flag)
```

# Function of confusion matrix

```{r}


# let's use this helper function that will return all the rates for future calculations
confusion_matrix <- function(d){
  data.frame(tp=nrow(d[d$class==1 & d$scored.class==1,]),
             tn=nrow(d[d$class==0 & d$scored.class==0,]),
             fp=nrow(d[d$class==0 & d$scored.class==1,]),
             fn=nrow(d[d$class==1 & d$scored.class==0,])
  )
}
accuracy<-function(d){
  f <- confusion_matrix(d)
  (f$tp+f$tn)/(f$tp+f$fp+f$tn+f$fn)
}

classification_error_rate<-function(d){
  f <- confusion_matrix(d)
  (f$fp+f$fn)/(f$tp+f$fp+f$tn+f$fn)
}

precision_c<-function(d){
  f <- confusion_matrix(d)
  (f$tp)/(f$tp+f$fp)
}

sensitivity_c<-function(d){
  f <- confusion_matrix(d)
  (f$tp)/(f$tp+f$fn)
}

specificity_c<-function(d){
  f <- confusion_matrix(d)
  (f$tn)/(f$tn+f$fp)
}


f1_score<-function(d){
  p<- precision_c(d)
  s<- sensitivity_c(d)
  2*p*s/(p+s)
}
```


# Predictions and Accuracy

```{r}

#predict 1
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict1>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel1<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict1)
plot(d_roc, main = "ROC with pROC")
```

```{r}

#predict 2
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict2>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel2<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict2)
plot(d_roc, main = "ROC with pROC")
```

```{r}

#predict 3
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict3>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel3<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict3)
plot(d_roc, main = "ROC with pROC")
```

```{r}

#predict 4
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict4>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel4<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict4)
plot(d_roc, main = "ROC with pROC")
```


```{r}

#predict 5
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict5>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel5<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict5)
plot(d_roc, main = "ROC with pROC")
```




```{r}
#predict 6
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict6>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel6<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict6)
plot(d_roc, main = "ROC with pROC")
```

```{r}

#predict 7
d<- data.frame(class=training_trans_eval_bin$TARGET_FLAG,scored.class=ifelse(predict7>0.5,1,0))

confusion_matrix(d)
Accuracy <- accuracy(d)
Error <- classification_error_rate(d)
Precision <- precision_c(d)
Sensitivity <- sensitivity_c(d)
Specificity <- specificity_c(d)
F1 <- f1_score(d)

BestFitModel7<- data.frame(Accuracy,Error,Precision,Sensitivity,Specificity,F1)

require("pROC")
d_roc <- roc(training_trans_eval_bin$TARGET_FLAG,predict7)
plot(d_roc, main = "ROC with pROC")
```

##Compare the Models to choose the best
```{r}
CompareBestFitModel=rbind(BestFitModel1,BestFitModel2,BestFitModel3,BestFitModel4,BestFitModel5,BestFitModel6,BestFitModel7)
colnames(CompareBestFitModel)=c("Accuracy","Error","Precision","Sensitivity","Specificity","F1")
rownames(CompareBestFitModel)=c("Model1","Model2","Model3","Model4","Model5","Model6","Model7")
CompareBestFitModel


```

# Conclusion
From the above analysis, we can deduce that the AUC ( Area Under Curve) for all the three models are very close to 1, which indicate that the model 4 is more specificity, sensitivity and accuracy.



